# -*- coding: utf-8 -*-
"""a4_Dong_111658846.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lVhiCoEzDxTF2O_90LUC6gttNIbrwxWO
"""

import json
import numpy as np
from sys import argv
import sys
import re
#import os # this is use to change to current work directory
#os.chdir("/".join(sys.argv[0].split("/")[:-1]))

import gensim.downloader as api
from gensim.utils import tokenize

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

import warnings # ignore warnings
warnings.filterwarnings("ignore")

sys.stdout = open('./a4_Dong_111658846_OUTPUT.txt', 'w',encoding='utf-8')

from google.colab import drive
drive.mount('/content/gdrive')

import os
os.chdir("/content/gdrive/My Drive/CSE354/")

class DOC_RNN(nn.Module):

    def __init__(self, embedding_dim, lstm_hidden_dim, number_of_labels):
        #Initialize all of the layers the forward method will use:
        super(DOC_RNN, self).__init__()

        # Input already has tokens mapped to vectors: no embedding layer needed.
        # For getting LSTM hidden states from embedding vectors:
        self.gru = nn.GRU(embedding_dim, lstm_hidden_dim)

        # Transforms the embedding to a vector of length number_of_tags
        self.linearClassifier = nn.Linear(lstm_hidden_dim, number_of_labels)

    def forward(self, X):
        #Defines how the LSTM will run:
        #X: a list of FloatTensors: [tensor([...]), tensor([...]), ...]
        #   each tensor contains a matrix of input embeddings

        doc_vecs = [] #will hold the final hidden state of each document of X.
        for doc in X: #doc is a FloatTensor of all input embeddings for a single sequence
            s, _ = self.gru(doc.unsqueeze(1))
            #s is now the outputs for all words in the doc
            doc_vecs.append(s[-1]) #represent the doc as the the final word output

        #turn the list of document vectors into a matrix (tensor) of size: num_docs x lstm_hidden_dim
        doc_vecs = torch.stack(doc_vecs).squeeze(1)
        #obtain scores for each doc
        doc_vecs = self.linearClassifier(doc_vecs)
        #change the scores to probabilities1
        yprobs = F.softmax(doc_vecs)
        return yprobs

class GRUModel(nn.Module): # not used, please ignore
    def __init__(self, in_dim, hidden_dim, n_layer, n_class):
        super(GRUModel, self).__init__()
        self.n_layer = n_layer
        self.hidden_dim = hidden_dim
        # (seq_len, batch, input_size)#n_layers是GRU的层数
        self.GRU = nn.GRU(in_dim, hidden_dim, n_layer, batch_first=True)
        self.linear = nn.Linear(hidden_dim, n_class)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        out, _ = self.GRU(x)
        out = out[:, -1, :]
        out = self.linear(out)
        out = self.sigmoid(out)
        return out

data = []
test = []

with open("./music_QA_train.json", 'r') as infile:
    data = json.load(infile)
with open("./music_QA_dev.json", 'r') as infile:
    test = json.load(infile)
word_embs = api.load('glove-wiki-gigaword-50')

for record in data:
    record['label'] = 1 if record['label'] else 0
    record['question_toks'] = list(tokenize(record['question'], lowercase=True))
    record['passage_toks'] = list(tokenize(record['passage'], lowercase=True))
    for i in range(len(record['question_toks'])):
        try:
            record['question_toks'][i] = word_embs[record['question_toks'][i]]
        except Exception:
            record['question_toks'][i] = word_embs['unk']
    for i in range(len(record['passage_toks'])):
        try:
            record['passage_toks'][i] = word_embs[record['passage_toks'][i]]
        except Exception:
            record['passage_toks'][i] = word_embs['unk']

for record in test:
    record['label'] = 1 if record['label'] else 0
    record['question_toks'] = list(tokenize(record['question'], lowercase=True))
    record['passage_toks'] = list(tokenize(record['passage'], lowercase=True))
    for i in range(len(record['question_toks'])):
        try:
            record['question_toks'][i] = word_embs[record['question_toks'][i]]
        except Exception:
            record['question_toks'][i] = word_embs['unk']
    for i in range(len(record['passage_toks'])):
        try:
            record['passage_toks'][i] = word_embs[record['passage_toks'][i]]
        except Exception:
            record['passage_toks'][i] = word_embs['unk']

net = DOC_RNN(50,30,2).cuda()
#net = GRUModel(50,3,1,2)
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.5)

for epoch in range(10):  # loop over the dataset multiple times
    running_loss = 0.0

    inputs = [torch.tensor(i['question_toks']).cuda() for i in data]
    labels = torch.tensor([i['label'] for i in data]).cuda()

    # forward + backward + optimize
    outputs = net(inputs)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()

    # print statistics
    running_loss += loss.item()
    print('[%d, %5d] loss: %.3f' %
          (epoch + 1, i + 1, running_loss / 50))
    running_loss = 0.0

print('Finished Training')

inputTest = [torch.tensor(i['question_toks']).cuda() for i in test]
labelTest = np.array([i['label'] for i in test])
yPred = net(inputs).cpu().detach().numpy()

result = []
for i in yPred:
    if i[0]>=i[1]:
        result.append(0)
    else:
        result.append(1)

count = 0
for i in range(labelTest.size):
    if result[i] == labelTest[i]:
        count+=1

print("Accuracy rate is:",count/labelTest.size)

"""---"""



#!pip install transformers

#!pip install datasets

from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from datasets import Dataset

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model =  AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2) # distilbert-base-uncased
model2 = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2) # passage model
model3 = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2) # both

train_dataset = Dataset.from_dict({k: [d[k] for d in data] for k in data[0]})
dev_dataset = Dataset.from_dict({k: [d[k] for d in test] for k in test[0]})

def preprocess_function(examples):
    return tokenizer(examples["question"], truncation=True,padding=True)

encoded_train = train_dataset.map(preprocess_function, 
batched=True,load_from_cache_file=False)
encoded_test = dev_dataset.map(preprocess_function, 
batched=True,load_from_cache_file=False)

columns_to_return = ['input_ids', 'label', 'attention_mask']
encoded_train.set_format(type='torch', columns=columns_to_return)
encoded_test.set_format(type='torch', columns=columns_to_return)

args = TrainingArguments(
    f"question",
    evaluation_strategy = "epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
)# Taken from Hugging Face Sequence Classification Tutorial

trainer = Trainer(
    model,
    args,
    train_dataset=encoded_train,
    eval_dataset=encoded_test,
    tokenizer=tokenizer,
)# Taken from Hugging Face Sequence Classification Tutorial

trainer.train()

# model.save_pretrained('./model/')

trainer.evaluate()

pred = trainer.predict(encoded_test)
lst = [0 if i[0]>i[1] else 1 for i in pred.predictions]
count=0
for i in range(len(lst)):
  if lst[i] == pred.label_ids[i]:
    count+=1
print("Accuraccy rate of question model is:",count/len(lst))

"""---"""

def preprocess_function(examples):
    return tokenizer(examples["passage"], truncation=True,padding=True)

encoded_train = train_dataset.map(preprocess_function, 
batched=True,load_from_cache_file=False)
encoded_test = dev_dataset.map(preprocess_function, 
batched=True,load_from_cache_file=False)

columns_to_return = ['input_ids', 'label', 'attention_mask']
encoded_train.set_format(type='torch', columns=columns_to_return)
encoded_test.set_format(type='torch', columns=columns_to_return)

args2 = TrainingArguments(
    f"passage",
    evaluation_strategy = "epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
)# Taken from Hugging Face Sequence Classification Tutorial

trainer2 = Trainer(
    model2,
    args2,
    train_dataset=encoded_train,
    eval_dataset=encoded_test,
    tokenizer=tokenizer,
)# Taken from Hugging Face Sequence Classification Tutorial

trainer2.train()

# model2.save_pretrained('./model2/')

trainer2.evaluate()

pred1 = trainer2.predict(encoded_test)
lst = [0 if i[0]>i[1] else 1 for i in pred1.predictions]
count=0
for i in range(len(lst)):
  if lst[i] == pred1.label_ids[i]:
    count+=1
print("Accuraccy rate of passage model is:",count/len(lst))

"""---"""

def preprocess_function(examples):
    return tokenizer(examples["question"],examples["passage"], truncation=True,padding=True,add_special_tokens=True)

encoded_train = train_dataset.map(preprocess_function, 
batched=True,load_from_cache_file=False)
encoded_test = dev_dataset.map(preprocess_function, 
batched=True,load_from_cache_file=False)

columns_to_return = ['input_ids', 'label', 'attention_mask']
encoded_train.set_format(type='torch', columns=columns_to_return)
encoded_test.set_format(type='torch', columns=columns_to_return)

args3 = TrainingArguments(
    f"passage",
    evaluation_strategy = "epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=3,
    weight_decay=0.01,
)# Taken from Hugging Face Sequence Classification Tutorial

trainer3 = Trainer(
    model3,
    args3,
    train_dataset=encoded_train,
    eval_dataset=encoded_test,
    tokenizer=tokenizer,
)# Taken from Hugging Face Sequence Classification Tutorial

trainer3.train()

trainer3.evaluate()

pred2 = trainer3.predict(encoded_test)
lst = [0 if i[0]>i[1] else 1 for i in pred2.predictions]
count=0
for i in range(len(lst)):
  if lst[i] == pred2.label_ids[i]:
    count+=1
print("Accuraccy rate of full question-passage model is:",count/len(lst))

"""---"""

question1 = "Is weather going to be change at constant rate?"
question2 = "Am I the most handsome man on the world?"
question3 = "Could you please say yes?"
q1 = tokenizer(question1, truncation=True,padding=True)
q2 = tokenizer(question2, truncation=True,padding=True)
q3 = tokenizer(question3, truncation=True,padding=True)

toTest = {
    'label':[0,0,0],
    'input_ids':[q1['input_ids'],q2['input_ids'],q3['input_ids']],
    'attention_mask':[q1['attention_mask'],q2['attention_mask'],q3['attention_mask']]
}
que3 = Dataset.from_dict(toTest)

prediction = ["True" if i[0]>i[1] else "False" for i in trainer3.predict(que3).predictions]

print("----------------------------------------------------------------------")
print("Three Questions:\n")
print("Question 1:",question1)
print("Answer 1:",prediction[0])
print("Question 2:",question2)
print("Answer 2:",prediction[1])
print("Question 3:",question3)
print("Answer 3:",prediction[2])
print("----------------------------------------------------------------------")

"""---"""

model4 = AutoModelForSequenceClassification.from_pretrained("bert-base-multilingual-uncased", num_labels=2) # fine-tune another large model

def preprocess_function(examples):
    return tokenizer(examples["question"],examples["passage"], truncation=True,padding=True,add_special_tokens=True)

encoded_train = train_dataset.map(preprocess_function, 
batched=True,load_from_cache_file=False)
encoded_test = dev_dataset.map(preprocess_function, 
batched=True,load_from_cache_file=False)

columns_to_return = ['input_ids', 'label', 'attention_mask']
encoded_train.set_format(type='torch', columns=columns_to_return)
encoded_test.set_format(type='torch', columns=columns_to_return)

args4 = TrainingArguments(
    f"passage",
    evaluation_strategy = "epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=3,
    weight_decay=0.01,
)# Taken from Hugging Face Sequence Classification Tutorial

trainer4 = Trainer(
    model4,
    args4,
    train_dataset=encoded_train,
    eval_dataset=encoded_test,
    tokenizer=tokenizer,
)# Taken from Hugging Face Sequence Classification Tutorial

trainer4.train()

trainer4.evaluate()

pred4 = trainer4.predict(encoded_test)
lst = [0 if i[0]>i[1] else 1 for i in pred4.predictions]
count=0
for i in range(len(lst)):
  if lst[i] == pred4.label_ids[i]:
    count+=1
print("Accuraccy rate of passage model is:",count/len(lst))

"""---"""

print("Now making prediction on Test set...")

final = []
with open("./music_QA_test.json", 'r') as infile:
    final = json.load(infile)
final_dataset = Dataset.from_dict({k: [d[k] for d in final] for k in final[0]})

def preprocess_function(examples):
    return tokenizer(examples["question"],examples["passage"], truncation=True,padding=True,add_special_tokens=True)

final_data = final_dataset.map(preprocess_function, 
batched=True,load_from_cache_file=False)
columns_to_return = ['input_ids', 'attention_mask']
final_data.set_format(type='torch', columns=columns_to_return)

pred_final = trainer.predict(final_data).predictions

pair = [0 if i[0]>i[1] else 1for i in pred_final]

import pandas as pd
df = pd.DataFrame(pair,columns=['label'], index=[i['idx'] for i in final_dataset])

df.reset_index().rename(columns={'index':'idx'}).to_csv("./Dong_Kaggle_OUTPUT.csv", index=False, sep=',')

print("Done. Please check .csv")

